#!@PYTHON_EXECUTABLE@

#

## Copyright 2012 MIT Haystack Observatory
##
## This file is part of Mark6 command and control file.
##
## M6_CC is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, version 2 of the License.
##
## M6_CC is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.    See the
## GNU General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with M6_CC.    If not, see <http://www.gnu.org/licenses/>.

'''
Author:     chester@haystack.mit.edu
Date:         9/07/2012
Description:

Implements the command and control of the Mark6 recording system
by issuing the appropriate VSI-S commands to the Mark6 cplane.
The methods used to invoke the M6_CC is to supply a file that
contains the XML description of the schedule to be executed.
'''
import socket
import os
import sys
import getopt
import string
import time
import json
import threading
import subprocess
from datetime import datetime, timedelta
import calendar
import logging
import xml.etree.ElementTree as ET
from hose import *

#global vars
logger = logging.getLogger("Hose_CC")
data_install_dir = "@DATA_INSTALL_DIR@"
log_install_dir = "@LOG_INSTALL_DIR@"
schedule_is_running = True

def log_to_db():

    #create interface and connect to (Westford) data base
    logfilename = os.path.join(log_install_dir, "status.log")
    dbinterface = wf_influxdb()

    command = "tail -f " + logfilename
    tail = subprocess.Popen([command], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    stripper = hstatuslog_stripper()

    run_loop = True
    while run_loop == True:
        line = tail.stdout.readline()
        if line:
            result_valid = stripper.process_line(line.strip())
            if result_valid is True:
                json_body = stripper.get_data_points()[0]
                print("Write points: {0}".format(json_body))
                dbinterface.client.write_points([json_body], protocol='json')
        if tail.poll() is not None or os.path.isfile(logfilename) is False or schedule_is_running is False:
            run_loop = False


def setup_logging(logfile):
    '''
    Setup the logging
    '''
    logger.setLevel(logging.DEBUG)
    # create file handler which logs even debug messages
    fh = logging.FileHandler(logfile)
    fh.setLevel(logging.DEBUG)
    # create console handler with a higher log level
    ch = logging.StreamHandler()
    ch.setLevel(logging.WARNING)
    # create formatter and add it to the handlers
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)
    # add the handlers to logger
    logger.addHandler(ch)
    logger.addHandler(fh)

def seconds_until(target):
    '''
    Returns seconds until the target time; >0 if target is in the future
    '''
    tim = calendar.timegm(time.strptime(target, '%Y%j%H%M%S'))
    now = calendar.timegm(time.gmtime())
    return tim - now

def args():
    '''
    Parse arguments and return xml file, mark6 host and port
    '''
    parms = {'-f':"test.xml",       # schedule file
             '-m':'127.0.0.1',      # hose host ip address
             '-p':12345            # hose host port
             }
    try:
        opts, pargs = getopt.getopt(sys.argv[1:], "f:m:p:")
    except getopt.GetoptError, msg:
        print msg
        sys.exit(1)

    for o,v in opts:
        parms[o] = v

    input_fn = str(parms['-f'])
    hose_host = str(parms['-m'])
    hose_port = int(parms['-p'])
    return input_fn, hose_host, hose_port

def hose_send(hose_s, st, hosecmd, where):
    '''
    This method sends commands to the mark6, with logging and
    error checking.  A null string on return implies success.
    '''
    global logger
    logger.info("%s - sent: %s", st, hosecmd.strip())
    try:
        hose_s.SendRecieveMessage(hosecmd)
    except Exception, e:
        logger.error("%s - send exception %s", st, str(e))
        return where + ', ' + str(e)
    logger.info("%s - send success (%s)", st, where)
    return ''

def vextime(jt):
    '''
    Convert YYYYDOYHHMMSS into vextime YYYYyDOYdHHhMMmSSs
    '''
    vx = jt[0:4] + 'y' + jt[4:7] + 'd' + \
        jt[7:9] + 'h' + jt[9:11] + 'm' + \
        jt[11:13] + 's'
    return vx

def as_datetime(jt):
    '''
    Convert YYYYDOYHHMMSS into datetime object
    '''
    year = int(jt[0:4])
    day = int(jt[4:7])
    hour = int(jt[7:9])
    minute = int(jt[9:11])
    second = int(jt[11:13])
    abstime = datetime(year,1,1) + timedelta( days=day-1, hours=hour, minutes=minute, seconds=second, microseconds=0)
    return abstime


def main(input_fn, hose_host, hose_port):
    '''
    Run the XML schedule from the input file at the specified host.
    '''
    global logger
    errmsg = ''
    proceed = False
    if os.path.isfile(input_fn):
        try:
            tree = ET.parse(input_fn)
            errmsg = ("Parsed input schedule file %s" % input_fn)
            proceed = True
        except Exception, e:
            errmsg = ("Unparsable %s: %s" % (input_fn, e))
    else:
        errmsg = ("Session XML file %s does not exist " % (input_fn))

    if proceed:
        root = tree.getroot()
        exp = root.get('name')
        st = root.get('station')
        exp_start = root.get('start')
        exp_end = root.get('end')
    else:
        st = 'xx'

    t = threading.Thread(name='log2db', target=log_to_db)
    t.start()

    #open up a meta-data database client
    dbclient = wf_influxdb()

    # --- start the log
    stamp = time.strftime('%Y.%j.%H.%M.%S',time.gmtime());
    logname = 'hose_CC-' + st + '-' + stamp + '.log'
    setup_logging(logname)
    logger.info('%s - log started', st)
    logger.info('%s - %s', st, errmsg)
    if not proceed:
        logger.info('%s - unable to continue', st)
        sys.exit(1)
    # --- number of scans in file
    numscans = len(tree.findall('scan'))
    logger.debug("%s - client %s port %d, scans %d",
        st, hose_host, hose_port, numscans)

    # --- connect to mk6
    hose_s = hclient()

    logger.info("%s - Exp %s, station %s", st, exp, st)
    logger.info("%s - starting %s, ending %s", st, exp_start, exp_end)

    scan_no = 0
    # --- for the remainder of the list process the scan information
    for scan in root.findall('scan'):
        scan_no += 1 
        # --- parse scan information
        exp = scan.get('experiment')
        src = scan.get('source')
        sc = scan.get('station_code')
        start_time = scan.get('start_time')
        delta = scan.get('duration')
        sn = scan.get('scan_name')
        logger.debug("%s - source %s starting %s dur %s scan_name %s",
            st, src, start_time, delta, sn)
        logger.debug("%s - scan %d of %d", st, scan_no, numscans)

        #source dunno starting 2018163135822 dur 47 scan_name 163-1358
        start_time_stamp = as_datetime(start_time)
        end_time_stamp = start_time_stamp + timedelta(seconds=int(delta))

        # --- get time from system
        dot_time = time.strftime('%Y%j%H%M%S',time.gmtime());
        logger.info("%s - current time %s ", st, dot_time)
        logger.info("%s - start_time   %s ", st, start_time)

        diff = seconds_until(start_time)
        if diff > -int(delta):
            # --- there is some time left in the scan
            logger.info("%s - hose record on (%d sec to go)", st, diff)
            hosecmd = "record=on:" + exp + ":" + src + ":" + sn + ":" + start_time + ":" + delta
            where = hose_send(hose_s, st, hosecmd, "record=on send")
            if where != '':
                break

            # --- sleep until after scan starts
            diff = seconds_until(start_time) + 2
            if diff > 0:
                logger.debug ("%s - sleeping for %d secs", st, diff)
                time.sleep(diff)

            # --- sleep until after done
            diff = seconds_until(start_time) + int(delta) + 1
            if diff > 0:
                logger.debug ("%s - sleeping for %d secs", st, diff)
                time.sleep(diff)
            else:
                logger.debug("%s - Not Sleeping during scan", st)

        
            hosecmd = "record=off"
            where = hose_send(hose_s, st, "record=off", "record=off")
            if where != '':
                break

            # --- make some allowance for recorder data to drain
            logger.debug ("%s - sleeping for %d secs", st, 2)
            time.sleep(2)
    
            #collect the meta data here, and dump it to a file
            #time arguments must be datetime objects
            #valid measurement names are:
            #digitizer_config
            #spectrometer_config
            #noise_diode_config
            #recording_status 
            #udc_status
            #source 
            #data_validity

            meta_data_filename = "meta_data.json"
            meta_data_filepath = os.path.join(data_install_dir, exp, sn, meta_data_filename)
            obj_list = []

            digi_config = dbclient.get_most_recent_measurement("digitizer_config", end_time_stamp)
            for x in digi_config:
                obj_list.append(x)
            spec_config = dbclient.get_most_recent_measurement("spectrometer_config", end_time_stamp)
            for x in spec_config:
                obj_list.append(x)
            noise_config = dbclient.get_most_recent_measurement("noise_diode_config", end_time_stamp)
            for x in noise_config:
                obj_list.append(x)
            udc_info = dbclient.get_most_recent_measurement("udc_status", end_time_stamp)
            for x in udc_info:
                obj_list.append(x)
            source_info = dbclient.get_most_recent_measurement("source", end_time_stamp)
            for x in source_info:
                obj_list.append(x)
            recording_info = dbclient.get_measurement_from_time_range("recording_status", start_time_stamp, end_time_stamp, 3)
            for x in recording_info:
                obj_list.append(x)
            data_validity_info = dbclient.get_measurement_from_time_range("data_validity", start_time_stamp, end_time_stamp, 3)
            for x in data_validity_info:
                obj_list.append(x)
            dump_dict_list_to_json_file(obj_list, meta_data_filepath)


            # --- nothing more to do for this scan
            logger.debug ("%s - sleeping for %d secs", st, 2)
            time.sleep(2)

            where = 'done'

	# --- it was too late for that scan...
        else:
            logger.warning("%s - dot time %s passed the start time %s",
                st, dot_time, start_time) 
            logger.warning("%s - skipping this scan...trying the next", st)
            where = 'skip'

	# --- bail if input file now missing
	if os.path.isfile(input_fn):
	    logger.debug ("%s - still have input fn %s", st, input_fn)
	else:
	    where = 'input file ' + input_fn + ' removed'
	    break

    schedule_is_running = False

    # --- parting words
    logger.info("%s - reached this place via '%s'", st, where)
    logger.info("%s - %s file processing complete ", st, input_fn)
    hose_s.Shutdown()

    t.join()

if __name__ == '__main__':
    fn, host, port = args()
    main(fn, host, port)
    sys.exit(0)


#####  ENDOF FILE
